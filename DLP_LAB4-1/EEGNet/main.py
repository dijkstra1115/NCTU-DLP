# -*- coding: utf-8 -*-
"""EEGNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u3WKIyXj4TpmucrT9G-4K8XXOBIJxz2m
"""

from dataloader import read_bci_data

import numpy as np
import random
import argparse
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR, MultiStepLR

class EEG(nn.Module):
    def __init__(self,act_func='ELU'):
        super(EEG, self).__init__()
        if act_func == 'ELU': self.act_func = nn.ReLU()
        if act_func == 'LeakyReLU': self.act_func = nn.LeakyReLU()
        if act_func == 'ReLU': self.act_func = nn.ReLU()
        self.firstconv = nn.Sequential(
            nn.Conv2d(1,16,kernel_size=(1,51),stride=(1,1),padding=(0,25),bias=False),
            nn.BatchNorm2d(16,eps=1e-5,momentum=0.1,affine=True,track_running_stats=True)
        )
        self.depthwiseConv = nn.Sequential(
            nn.Conv2d(16,32,kernel_size=(2,1),stride=(1,1),groups=16,bias=False),
            nn.BatchNorm2d(32,eps=1e-5,momentum=0.1,affine=True,track_running_stats=True),
            self.act_func,
            nn.AvgPool2d(kernel_size=(1,4),stride=(1,4),padding=0),
            nn.Dropout(p=0.25)
        )
        self.seperableConv = nn.Sequential(
            nn.Conv2d(32,32,kernel_size=(1,15),stride=(1,1),padding=(0,7),bias=False),
            nn.BatchNorm2d(32,eps=1e-5,momentum=0.1,affine=True,track_running_stats=True),
            self.act_func,
            nn.AvgPool2d(kernel_size=(1,8),stride=(1,8),padding=0),
            nn.Dropout(p=0.25),
            nn.Flatten(),
			      nn.Linear(in_features=736,out_features=2,bias=True)
        )
    def forward(self,x):
        x = self.firstconv(x)
        x = self.depthwiseConv(x)
        x = self.seperableConv(x)
        return x

class DeepConvNet(nn.Module):
    def __init__(self,act_func='ELU'):
        super(DeepConvNet, self).__init__()
        if act_func == 'ELU': self.act_func = nn.ReLU()
        if act_func == 'LeakyReLU': self.act_func = nn.LeakyReLU()
        if act_func == 'ReLU': self.act_func = nn.ReLU()
        C, T, N = 2, 750, 2
        self.conv0 = nn.Sequential(
          nn.Conv2d(in_channels=1, out_channels=25, kernel_size=(1,5)),
          nn.Conv2d(in_channels=25, out_channels=25, kernel_size=(C,1)),
          nn.BatchNorm2d(25, eps=1e-5, momentum=0.1)
        )
        self.conv1 = nn.Sequential(
          nn.MaxPool2d(kernel_size=(1,2)),
          nn.Dropout(p=0.5),
          nn.Conv2d(in_channels=25, out_channels=50, kernel_size=(1,5)),
          nn.BatchNorm2d(50, eps=1e-5, momentum=0.1)
        )
        self.conv2 = nn.Sequential(
          nn.MaxPool2d(kernel_size=(1,2)),
          nn.Dropout(p=0.5),
          nn.Conv2d(in_channels=50, out_channels=100, kernel_size=(1,5)),
          nn.BatchNorm2d(100, eps=1e-5, momentum=0.1)
        )
        self.conv3 = nn.Sequential(
          nn.MaxPool2d(kernel_size=(1,2)),
          nn.Dropout(p=0.5),
          nn.Conv2d(in_channels=100, out_channels=200, kernel_size=(1,5)),
          nn.BatchNorm2d(200, eps=1e-5, momentum=0.1)
        )
        self.conv4 = nn.Sequential(
          nn.MaxPool2d(kernel_size=(1,2)),
          nn.Dropout(p=0.5),
          nn.Flatten(),
          nn.Linear(in_features=8600,out_features=2)
        )

    def forward(self,x):
        x = self.conv0(x)
        x = self.act_func(x)
        x = self.conv1(x)
        x = self.act_func(x)
        x = self.conv2(x)
        x = self.act_func(x)
        x = self.conv3(x)
        x = self.act_func(x)
        x = self.conv4(x)
        return x

i = 0
def train( model, train_data, train_label, optimizer, batchsize):
    global i
    count = 0
    model.train()
    while count<1080:
        data = torch.cuda.FloatTensor( train_data[i:i+batchsize] )
        target = torch.cuda.LongTensor( train_label[i:i+batchsize] )
        optimizer.zero_grad()
        output = model(data)
        loss = nn.CrossEntropyLoss()
        loss = loss(output, target)
        loss.backward()
        optimizer.step()

        i = (i+batchsize)%1080
        count += batchsize

def test(model, test_data, test_label):
    model.eval()
    data = torch.cuda.FloatTensor( test_data )
    target = torch.cuda.LongTensor( test_label )
    output = model(data)
    loss = nn.CrossEntropyLoss()
    test_loss = loss(output, target)  # sum up batch loss
    pred = output.argmax(dim=1)  # get the index of the max log-probability
    correct = 0
    for i,pred_ans in enumerate(pred):
        if pred[i] == target[i]: correct += 1
    return test_loss.item()/1080.0 , correct/1080.0

# lr = 0.001
# weight_decay = 0.01
batchsize = 256
epochs = 501

# torch.manual_seed(1)
device = torch.device('cuda:0')
train_data, train_label, test_data, test_label = read_bci_data()



for task in ['EEG', 'DeepConvNet']:
    plt_array_loss = []
    plt_array_accuracy = []
    #['ReLU', 'LeakyReLU', 'ELU']
    for act_func in ['ReLU', 'LeakyReLU', 'ELU']:
        print(str(task+'_'+act_func))
        plt_array_loss_tmp = []
        plt_array_accuracy_tmp = []

        if task == 'EEG':
            model = EEG(act_func=act_func)
            # model=EEGNet(nn.LeakyReLU())
        elif task == 'DeepConvNet':
            model = DeepConvNet(act_func=act_func)

        model.to(device)
        optimizer = optim.Adam(model.parameters(),lr = 0.001,weight_decay = 0.01)
        # scheduler = StepLR(optimizer, step_size=100, gamma=0.985)
        for epoch in range(epochs):
            train(model, train_data, train_label, optimizer, batchsize)
            train_loss, train_correct = test(model, train_data, train_label)
            test_loss, test_correct = test(model, test_data, test_label)
            # scheduler.step()

            plt_array_accuracy_tmp.append(train_correct*100)
            plt_array_accuracy_tmp.append(test_correct*100)
            plt_array_loss_tmp.append(train_loss)
            plt_array_loss_tmp.append(test_loss)

            if epoch%100 == 0: print('training epoch= ',epoch,' loss= ',train_loss,' correct= ',train_correct)
            if epoch%100 == 0: print('testing epoch= ',epoch,' loss= ',test_loss,' correct= ',test_correct)

        plt_array_accuracy.append(plt_array_accuracy_tmp)
        plt_array_loss.append(plt_array_loss_tmp)
        # torch.save(model.state_dict(), str('hw3'+task+act_func+testset+'.pt'))


    for arr in plt_array_accuracy: plt.plot(arr)
    plt.title(str("Activation Functions comparision ("+task+')'))
    plt.grid()
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy(%)')
    plt.legend(['relu_train', 'relu_test', 'leaky_relu_train', 'leaky_relu_test', 'elu_train', 'elu_test',])
    plt.savefig(str(task+'_accuracy.png'))
    plt.close()
    plt.show()

    for arr in plt_array_loss: plt.plot(arr)
    plt.grid()
    plt.title(str("Learning curve comparision ("+task+')'))
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(['relu_train', 'relu_test', 'leaky_relu_train', 'leaky_relu_test', 'elu_train', 'elu_test',])
    plt.savefig(str(task+'_loss.png'))
    plt.close()
    plt.show()

